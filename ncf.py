# -*- coding: utf-8 -*-
"""fix_ncf_predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GtXQpGQq9Cd-olLooz96qh8B8fVLFwWL
"""

# -*- coding: utf-8 -*-
"""
Step 2: Retrain NCF Model with Improved Settings
Fixes constant prediction issue
"""

import pandas as pd
import numpy as np
import pickle
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math

print("\n" + "="*60)
print("STEP 2: RETRAINING NCF MODEL")
print("="*60)

# Load data
print("\nLoading training data...")
train_df = pd.read_csv('/content/drive/MyDrive/Recommendation/requirements/train_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/Recommendation/requirements/test_data.csv')
print(f"✓ Training: {len(train_df):,} records")
print(f"✓ Test: {len(test_df):,} records")

# Get dimensions
n_users = max(train_df['user_id_encoded'].max(), test_df['user_id_encoded'].max()) + 1
n_movies = max(train_df['movie_id_encoded'].max(), test_df['movie_id_encoded'].max()) + 1

print(f"\nDimensions:")
print(f"  Users: {n_users}")
print(f"  Movies: {n_movies}")

# Prepare data
X_train_user = train_df['user_id_encoded'].values
X_train_movie = train_df['movie_id_encoded'].values
y_train = train_df['rating'].values

X_test_user = test_df['user_id_encoded'].values
X_test_movie = test_df['movie_id_encoded'].values
y_test = test_df['rating'].values

# Normalize ratings
print("\nNormalizing ratings to [0, 1]...")
scaler = MinMaxScaler()
y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test_scaled = scaler.transform(y_test.reshape(-1, 1)).flatten()

print(f"Training ratings: mean={y_train.mean():.2f}, std={y_train.std():.2f}")
print(f"Scaled: mean={y_train_scaled.mean():.4f}, std={y_train_scaled.std():.4f}")

# Build model
print("\n" + "="*60)
print("BUILDING NCF MODEL (IMPROVED)")
print("="*60)

embedding_dim = 100
dense_units = [256, 128, 64]

print(f"Architecture:")
print(f"  Embedding dimension: {embedding_dim}")
print(f"  Dense layers: {dense_units}")
print(f"  Batch normalization: YES")
print(f"  Dropout: 0.2")

# Clear session
keras.backend.clear_session()

# Inputs
user_input = layers.Input(shape=(1,), name='user_input', dtype='int32')
movie_input = layers.Input(shape=(1,), name='movie_input', dtype='int32')

# Embeddings with better initialization
user_embedding = layers.Embedding(
    input_dim=n_users,
    output_dim=embedding_dim,
    embeddings_initializer='glorot_uniform',
    name='user_embedding'
)(user_input)
user_vec = layers.Flatten()(user_embedding)

movie_embedding = layers.Embedding(
    input_dim=n_movies,
    output_dim=embedding_dim,
    embeddings_initializer='glorot_uniform',
    name='movie_embedding'
)(movie_input)
movie_vec = layers.Flatten()(movie_embedding)

# Concatenate
concat = layers.Concatenate()([user_vec, movie_vec])

# Dense layers with batch normalization
x = concat
for i, units in enumerate(dense_units):
    x = layers.Dense(
        units,
        activation='relu',
        kernel_initializer='he_normal',
        name=f'dense_{i+1}'
    )(x)
    x = layers.BatchNormalization(name=f'bn_{i+1}')(x)
    x = layers.Dropout(0.2, name=f'dropout_{i+1}')(x)

# Output
output = layers.Dense(1, activation='sigmoid', name='output')(x)

# Create model
ncf_model = keras.Model(
    inputs=[user_input, movie_input],
    outputs=output,
    name='NCF_Model'
)

# Compile with LOWER learning rate
ncf_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Lower LR
    loss='mse',
    metrics=['mae']
)

print("\n" + "="*60)
print("MODEL SUMMARY")
print("="*60)
ncf_model.summary()

# Training
print("\n" + "="*60)
print("TRAINING")
print("="*60)

batch_size = 512
epochs = 50

print(f"Parameters:")
print(f"  Batch size: {batch_size}")
print(f"  Epochs: {epochs}")
print(f"  Learning rate: 0.0005")
print(f"  Early stopping: patience=5")
print(f"  LR reduction: patience=3")

# Callbacks
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=0.00001,
    verbose=1
)

# Train
print("\nStarting training...")
history = ncf_model.fit(
    [X_train_user, X_train_movie],
    y_train_scaled,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.1,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

print("\n✓ Training complete!")

# Analyze training
print("\n" + "="*60)
print("TRAINING ANALYSIS")
print("="*60)

final_train_loss = history.history['loss'][-1]
final_val_loss = history.history['val_loss'][-1]
final_train_mae = history.history['mae'][-1]
final_val_mae = history.history['val_mae'][-1]

print(f"Final metrics:")
print(f"  Train loss: {final_train_loss:.4f}")
print(f"  Val loss: {final_val_loss:.4f}")
print(f"  Train MAE: {final_train_mae:.4f}")
print(f"  Val MAE: {final_val_mae:.4f}")

if final_train_loss < 0.015 and final_val_loss < 0.020:
    print("\n✅ Training looks good!")
else:
    print("\n⚠️ Training may need more epochs or lower learning rate")

# Test predictions
print("\n" + "="*60)
print("TEST SET EVALUATION")
print("="*60)

test_loss, test_mae = ncf_model.evaluate(
    [X_test_user, X_test_movie],
    y_test_scaled,
    batch_size=batch_size,
    verbose=0
)

print(f"Test metrics (scaled):")
print(f"  Loss (MSE): {test_loss:.4f}")
print(f"  MAE: {test_mae:.4f}")

# Get predictions
print("\nGenerating predictions...")
test_pred_scaled = ncf_model.predict(
    [X_test_user, X_test_movie],
    batch_size=batch_size,
    verbose=0
)

test_pred = scaler.inverse_transform(test_pred_scaled).flatten()

# Check distribution
print("\n" + "="*60)
print("PREDICTION DISTRIBUTION CHECK")
print("="*60)

print(f"Scaled predictions:")
print(f"  Mean: {test_pred_scaled.mean():.4f}")
print(f"  Std: {test_pred_scaled.std():.4f}")
print(f"  Min: {test_pred_scaled.min():.4f}")
print(f"  Max: {test_pred_scaled.max():.4f}")
print(f"  Unique: {len(np.unique(test_pred_scaled))}")

print(f"\nUnscaled predictions:")
print(f"  Mean: {test_pred.mean():.2f}")
print(f"  Std: {test_pred.std():.2f}")
print(f"  Min: {test_pred.min():.2f}")
print(f"  Max: {test_pred.max():.2f}")
print(f"  Unique: {len(np.unique(test_pred))}")

# Calculate RMSE/MAE on original scale
rmse = math.sqrt(mean_squared_error(y_test, test_pred))
mae = mean_absolute_error(y_test, test_pred)

print(f"\nPerformance (original scale):")
print(f"  RMSE: {rmse:.4f} (Paper target: 0.935)")
print(f"  MAE: {mae:.4f} (Paper target: 0.738)")

# Check if predictions are varied
if test_pred_scaled.std() < 0.05:
    print("\n❌ PROBLEM: Predictions are too constant!")
    print("   Model may need:")
    print("   - Even lower learning rate (0.0001)")
    print("   - More epochs (100)")
    print("   - Different architecture")
else:
    print(f"\n✅ SUCCESS: Model produces varied predictions! (std={test_pred_scaled.std():.4f})")

# Sample predictions
print("\n" + "="*60)
print("SAMPLE PREDICTIONS")
print("="*60)

sample_indices = np.random.choice(len(y_test), 10, replace=False)
print(f"{'User':>6} {'Movie':>6} {'Actual':>7} {'Predicted':>10} {'Error':>6}")
print("-" * 42)
for idx in sample_indices:
    user = X_test_user[idx]
    movie = X_test_movie[idx]
    actual = y_test[idx]
    predicted = test_pred[idx]
    error = abs(actual - predicted)
    print(f"{user:6d} {movie:6d} {actual:7.1f} {predicted:10.2f} {error:6.2f}")

# Save model
print("\n" + "="*60)
print("SAVING MODEL")
print("="*60)

ncf_model.save('/content/drive/MyDrive/Recommendation/requirements/ncf_model_retrained.h5')
print("✓ Saved: ncf_model_retrained.h5")

with open('/content/drive/MyDrive/Recommendation/requirements/ncf_scaler_retrained.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("✓ Saved: ncf_scaler_retrained.pkl")

# Final verdict
print("\n" + "="*60)
print("NCF RETRAINING COMPLETE!")
print("="*60)

if rmse < 1.0 and test_pred_scaled.std() > 0.05:
    print("\n✅ Model is working correctly!")
    print("   RMSE < 1.0 and predictions are varied")
    print("\nNext: Run 3_final_fusion.py")
else:
    print("\n⚠️ Model may need further tuning")
    print(f"   RMSE: {rmse:.4f} (target: < 1.0)")
    print(f"   Std: {test_pred_scaled.std():.4f} (target: > 0.05)")
    print("\nConsider:")
    print("   - Lower learning rate (0.0001)")
    print("   - More epochs (100)")
    print("   - Check data quality")

from google.colab import drive
drive.mount('/content/drive')