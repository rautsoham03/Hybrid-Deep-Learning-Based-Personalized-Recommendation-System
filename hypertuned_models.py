# -*- coding: utf-8 -*-
"""hypertuned models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bjfH7gE-yG-Q4fFnFA6FZsoMbjn007aG
"""

!pip install surprise

!pip install 'numpy<2'

"""
Complete Hyperparameter Tuning Script for All Models
Tunes all components and saves them as pickle/h5 files
"""

import pandas as pd
import numpy as np
import pickle
import json
import math
from datetime import datetime

# Surprise for SVD
from surprise import Dataset, Reader, SVD
from surprise.model_selection import GridSearchCV, train_test_split

# TensorFlow for Neural Models
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sklearn utilities
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics.pairwise import cosine_similarity

print("\n" + "="*80)
print("COMPREHENSIVE HYPERPARAMETER TUNING FOR HYBRID RECOMMENDATION SYSTEM")
print("="*80)
print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# ============================================================
# LOAD DATA
# ============================================================
print("\n" + "="*80)
print("STEP 1: LOADING DATA")
print("="*80)

train_df = pd.read_csv('/content/drive/MyDrive/Recommendation/success/requirements/train_data.csv')
test_df = pd.read_csv('/content/drive/MyDrive/Recommendation/success/requirements/test_data.csv')

train_df['rating_time'] = pd.to_datetime(train_df['rating_time'])
test_df['rating_time'] = pd.to_datetime(test_df['rating_time'])

print(f"✓ Loaded {len(train_df):,} training records")
print(f"✓ Loaded {len(test_df):,} test records")

n_users = max(train_df['user_id_encoded'].max(), test_df['user_id_encoded'].max()) + 1
n_movies = max(train_df['movie_id_encoded'].max(), test_df['movie_id_encoded'].max()) + 1

print(f"✓ Total users: {n_users}")
print(f"✓ Total movies: {n_movies}")

# Store all results
tuning_results = {}

# ============================================================
# 1. SVD MODEL TUNING
# ============================================================
print("\n" + "="*80)
print("STEP 2: TUNING SVD MODEL (Matrix Factorization)")
print("="*80)

reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))
data = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)

param_grid_svd = {
    'n_factors': [50, 100, 150, 200],
    'n_epochs': [20, 40, 60],
    'lr_all': [0.001, 0.002, 0.005, 0.01],
    'reg_all': [0.02, 0.05, 0.1, 0.2]
}

print(f"\nTesting {np.prod([len(v) for v in param_grid_svd.values()])} combinations...")
print("This may take 10-20 minutes...\n")

gs_svd = GridSearchCV(
    SVD,
    param_grid_svd,
    measures=['rmse', 'mae'],
    cv=3,
    n_jobs=-1,
    joblib_verbose=1
)
gs_svd.fit(data)

print(f"\n✓ Best RMSE: {gs_svd.best_score['rmse']:.4f}")
print(f"✓ Best MAE: {gs_svd.best_score['mae']:.4f}")
print(f"✓ Best parameters:")
for key, value in gs_svd.best_params['rmse'].items():
    print(f"    {key}: {value}")

# Get best model and retrain on full training set
best_svd_params = gs_svd.best_params['rmse']
trainset_full = data.build_full_trainset()
best_svd_model = SVD(**best_svd_params)
best_svd_model.fit(trainset_full)

# Save SVD model
svd_save_path = '/content/drive/MyDrive/Recommendation/success/svd_model_tuned.pkl'
with open(svd_save_path, 'wb') as f:
    pickle.dump(best_svd_model, f)
print(f"\n✓ Saved tuned SVD model to: {svd_save_path}")

tuning_results['svd'] = {
    'best_params': best_svd_params,
    'best_rmse': gs_svd.best_score['rmse'],
    'best_mae': gs_svd.best_score['mae']
}

# ============================================================
# 2. COLLABORATIVE FILTERING K-VALUE TUNING
# ============================================================
print("\n" + "="*80)
print("STEP 3: TUNING COLLABORATIVE FILTERING (K-VALUES)")
print("="*80)

# Load CF model
with open('/content/drive/MyDrive/Recommendation/success/requirements/cf_model_fixed.pkl', 'rb') as f:
    cf_data = pickle.load(f)

class CollaborativeFiltering:
    def __init__(self, data):
        self.user_item_matrix = data['user_item_matrix']
        self.user_similarity = data['user_similarity']
        self.item_similarity = data['item_similarity']
        self.global_mean = data['global_mean']

    def predict_user_based(self, user_id, movie_id, k=20):
        try:
            if self.user_similarity is None:
                return self.global_mean
            if user_id not in self.user_item_matrix.index:
                return self.global_mean
            if movie_id not in self.user_item_matrix.columns:
                return self.global_mean

            user_idx = self.user_item_matrix.index.get_loc(user_id)
            movie_idx = self.user_item_matrix.columns.get_loc(movie_id)
            sim_scores = self.user_similarity[user_idx]
            similar_user_indices = np.argsort(sim_scores)[::-1][1:k+1]

            ratings, weights = [], []
            for idx in similar_user_indices:
                rating = self.user_item_matrix.iloc[idx, movie_idx]
                if rating > 0:
                    ratings.append(rating)
                    weights.append(sim_scores[idx])

            if len(ratings) == 0 or sum(weights) == 0:
                return self.global_mean
            return np.average(ratings, weights=weights)
        except:
            return self.global_mean

    def predict_item_based(self, user_id, movie_id, k=20):
        try:
            if self.item_similarity is None:
                return self.global_mean
            if user_id not in self.user_item_matrix.index:
                return self.global_mean
            if movie_id not in self.user_item_matrix.columns:
                return self.global_mean

            user_idx = self.user_item_matrix.index.get_loc(user_id)
            movie_idx = self.user_item_matrix.columns.get_loc(movie_id)
            sim_scores = self.item_similarity[movie_idx]
            similar_item_indices = np.argsort(sim_scores)[::-1][1:k+1]

            ratings, weights = [], []
            for idx in similar_item_indices:
                rating = self.user_item_matrix.iloc[user_idx, idx]
                if rating > 0:
                    ratings.append(rating)
                    weights.append(sim_scores[idx])

            if len(ratings) == 0 or sum(weights) == 0:
                return self.global_mean
            return np.average(ratings, weights=weights)
        except:
            return self.global_mean

cf_model = CollaborativeFiltering(cf_data)

# Test different k values
k_values = [5, 10, 15, 20, 30, 40, 50, 75, 100]
test_subset = test_df.sample(min(2000, len(test_df)), random_state=42)

print("\nTesting k values for User-based CF:")
best_k_user = None
best_rmse_user = float('inf')

for k in k_values:
    preds = []
    actuals = []
    for _, row in test_subset.iterrows():
        pred = cf_model.predict_user_based(row['userId'], row['movieId'], k=k)
        preds.append(pred)
        actuals.append(row['rating'])

    rmse = math.sqrt(mean_squared_error(actuals, preds))
    mae = mean_absolute_error(actuals, preds)
    print(f"  k={k:3d}: RMSE={rmse:.4f}, MAE={mae:.4f}")

    if rmse < best_rmse_user:
        best_rmse_user = rmse
        best_k_user = k

print(f"\n✓ Best k for user-based CF: {best_k_user} (RMSE: {best_rmse_user:.4f})")

print("\nTesting k values for Item-based CF:")
best_k_item = None
best_rmse_item = float('inf')

for k in k_values:
    preds = []
    actuals = []
    for _, row in test_subset.iterrows():
        pred = cf_model.predict_item_based(row['userId'], row['movieId'], k=k)
        preds.append(pred)
        actuals.append(row['rating'])

    rmse = math.sqrt(mean_squared_error(actuals, preds))
    mae = mean_absolute_error(actuals, preds)
    print(f"  k={k:3d}: RMSE={rmse:.4f}, MAE={mae:.4f}")

    if rmse < best_rmse_item:
        best_rmse_item = rmse
        best_k_item = k

print(f"\n✓ Best k for item-based CF: {best_k_item} (RMSE: {best_rmse_item:.4f})")

# Save CF tuning results
cf_tuning = {
    'best_k_user': best_k_user,
    'best_k_item': best_k_item,
    'best_rmse_user': best_rmse_user,
    'best_rmse_item': best_rmse_item
}

cf_save_path = '/content/drive/MyDrive/Recommendation/success/cf_tuning_results.pkl'
with open(cf_save_path, 'wb') as f:
    pickle.dump(cf_tuning, f)
print(f"\n✓ Saved CF tuning results to: {cf_save_path}")

tuning_results['cf'] = cf_tuning

# ============================================================
# 3. CONTENT-BASED FILTERING K-VALUE TUNING
# ============================================================
print("\n" + "="*80)
print("STEP 4: TUNING CONTENT-BASED FILTERING")
print("="*80)

# Load CBF model
with open('/content/drive/MyDrive/Recommendation/success/requirements/cbf_model.pkl', 'rb') as f:
    cbf_data = pickle.load(f)

def get_cbf_prediction(user_id, movie_id, cbf_data, train_df, k=20):
    try:
        if movie_id not in cbf_data['movie_idx_map']:
            return cbf_data['global_mean']

        movie_idx = cbf_data['movie_idx_map'][movie_id]
        content_similarity = cbf_data['content_similarity']
        user_ratings = train_df[train_df['userId'] == user_id]

        if len(user_ratings) == 0:
            return cbf_data['global_mean']

        ratings, weights = [], []
        for _, rated_movie in user_ratings.iterrows():
            if rated_movie['movieId'] in cbf_data['movie_idx_map']:
                rated_idx = cbf_data['movie_idx_map'][rated_movie['movieId']]
                similarity = content_similarity[movie_idx, rated_idx]
                if similarity > 0:
                    ratings.append(rated_movie['rating'])
                    weights.append(similarity)

        if len(ratings) == 0 or sum(weights) == 0:
            return cbf_data['global_mean']

        if len(ratings) > k:
            top_k_indices = np.argsort(weights)[-k:]
            ratings = [ratings[i] for i in top_k_indices]
            weights = [weights[i] for i in top_k_indices]

        return np.average(ratings, weights=weights)
    except:
        return cbf_data['global_mean']

print("\nTesting k values for Content-Based Filtering:")
k_values_cbf = [10, 20, 30, 40, 50, 75, 100]
best_k_cbf = None
best_rmse_cbf = float('inf')

for k in k_values_cbf:
    preds = []
    actuals = []
    for _, row in test_subset.iterrows():
        pred = get_cbf_prediction(row['userId'], row['movieId'], cbf_data, train_df, k=k)
        preds.append(pred)
        actuals.append(row['rating'])

    rmse = math.sqrt(mean_squared_error(actuals, preds))
    mae = mean_absolute_error(actuals, preds)
    print(f"  k={k:3d}: RMSE={rmse:.4f}, MAE={mae:.4f}")

    if rmse < best_rmse_cbf:
        best_rmse_cbf = rmse
        best_k_cbf = k

print(f"\n✓ Best k for CBF: {best_k_cbf} (RMSE: {best_rmse_cbf:.4f})")

cbf_tuning = {
    'best_k': best_k_cbf,
    'best_rmse': best_rmse_cbf
}

cbf_save_path = '/content/drive/MyDrive/Recommendation/success/cbf_tuning_results.pkl'
with open(cbf_save_path, 'wb') as f:
    pickle.dump(cbf_tuning, f)
print(f"\n✓ Saved CBF tuning results to: {cbf_save_path}")

tuning_results['cbf'] = cbf_tuning

# ============================================================
# 4. NCF MODEL TUNING
# ============================================================
print("\n" + "="*80)
print("STEP 5: TUNING NCF MODEL (Neural Collaborative Filtering)")
print("="*80)

X_train_user = train_df['user_id_encoded'].values
X_train_movie = train_df['movie_id_encoded'].values
y_train = train_df['rating'].values

X_test_user = test_df['user_id_encoded'].values
X_test_movie = test_df['movie_id_encoded'].values
y_test = test_df['rating'].values

# Normalize ratings
scaler_ncf = MinMaxScaler()
y_train_scaled = scaler_ncf.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test_scaled = scaler_ncf.transform(y_test.reshape(-1, 1)).flatten()

# Create validation split
val_size = int(0.1 * len(X_train_user))
X_train_user_sub = X_train_user[:-val_size]
X_train_movie_sub = X_train_movie[:-val_size]
y_train_scaled_sub = y_train_scaled[:-val_size]

X_val_user = X_train_user[-val_size:]
X_val_movie = X_train_movie[-val_size:]
y_val_scaled = y_train_scaled[-val_size:]

def build_ncf_model(embedding_dim, dense_units, learning_rate, dropout_rate, n_users, n_movies):
    keras.backend.clear_session()

    user_input = layers.Input(shape=(1,), name='user_input', dtype='int32')
    movie_input = layers.Input(shape=(1,), name='movie_input', dtype='int32')

    user_embedding = layers.Embedding(
        n_users, embedding_dim,
        embeddings_initializer='glorot_uniform',
        name='user_embedding'
    )(user_input)
    user_vec = layers.Flatten()(user_embedding)

    movie_embedding = layers.Embedding(
        n_movies, embedding_dim,
        embeddings_initializer='glorot_uniform',
        name='movie_embedding'
    )(movie_input)
    movie_vec = layers.Flatten()(movie_embedding)

    concat = layers.Concatenate()([user_vec, movie_vec])

    x = concat
    for units in dense_units:
        x = layers.Dense(units, activation='relu', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(dropout_rate)(x)

    output = layers.Dense(1, activation='sigmoid', name='output')(x)

    model = keras.Model(inputs=[user_input, movie_input], outputs=output)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='mse',
        metrics=['mae']
    )
    return model

# Hyperparameter grid for NCF
ncf_configs = [
    {'embedding_dim': 100, 'dense_units': [256, 128, 64], 'learning_rate': 0.0005, 'dropout_rate': 0.2, 'batch_size': 512},
    {'embedding_dim': 150, 'dense_units': [512, 256, 128], 'learning_rate': 0.0003, 'dropout_rate': 0.2, 'batch_size': 512},
    {'embedding_dim': 100, 'dense_units': [256, 128, 64], 'learning_rate': 0.001, 'dropout_rate': 0.3, 'batch_size': 256},
    {'embedding_dim': 150, 'dense_units': [256, 128, 64], 'learning_rate': 0.0005, 'dropout_rate': 0.25, 'batch_size': 512},
    {'embedding_dim': 200, 'dense_units': [512, 256, 128, 64], 'learning_rate': 0.0003, 'dropout_rate': 0.2, 'batch_size': 512},
]

print(f"\nTesting {len(ncf_configs)} NCF configurations...")
print("This may take 30-60 minutes...\n")

best_ncf_rmse = float('inf')
best_ncf_config = None
best_ncf_model = None

for i, config in enumerate(ncf_configs):
    print(f"\nConfiguration {i+1}/{len(ncf_configs)}:")
    print(f"  Embedding: {config['embedding_dim']}, Dense: {config['dense_units']}")
    print(f"  LR: {config['learning_rate']}, Dropout: {config['dropout_rate']}, Batch: {config['batch_size']}")

    model = build_ncf_model(
        config['embedding_dim'],
        config['dense_units'],
        config['learning_rate'],
        config['dropout_rate'],
        n_users,
        n_movies
    )

    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True,
        verbose=0
    )

    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=0.00001,
        verbose=0
    )

    history = model.fit(
        [X_train_user_sub, X_train_movie_sub],
        y_train_scaled_sub,
        batch_size=config['batch_size'],
        epochs=50,
        validation_data=([X_val_user, X_val_movie], y_val_scaled),
        callbacks=[early_stopping, reduce_lr],
        verbose=0
    )

    # Evaluate
    val_pred = model.predict([X_val_user, X_val_movie], verbose=0)
    val_pred_unscaled = scaler_ncf.inverse_transform(val_pred).flatten()
    y_val_unscaled = scaler_ncf.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()

    rmse = math.sqrt(mean_squared_error(y_val_unscaled, val_pred_unscaled))
    mae = mean_absolute_error(y_val_unscaled, val_pred_unscaled)

    print(f"  Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}")
    print(f"  Epochs trained: {len(history.history['loss'])}")

    if rmse < best_ncf_rmse:
        best_ncf_rmse = rmse
        best_ncf_config = config
        best_ncf_model = model
        print("  ✓ New best model!")

print(f"\n✓ Best NCF RMSE: {best_ncf_rmse:.4f}")
print(f"✓ Best configuration:")
for key, value in best_ncf_config.items():
    print(f"    {key}: {value}")

# Retrain best model on full training data
print("\nRetraining best NCF model on full training data...")
final_ncf_model = build_ncf_model(
    best_ncf_config['embedding_dim'],
    best_ncf_config['dense_units'],
    best_ncf_config['learning_rate'],
    best_ncf_config['dropout_rate'],
    n_users,
    n_movies
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor='loss',
    patience=5,
    restore_best_weights=True
)

final_ncf_model.fit(
    [X_train_user, X_train_movie],
    y_train_scaled,
    batch_size=best_ncf_config['batch_size'],
    epochs=50,
    callbacks=[early_stopping],
    verbose=1
)

# Save NCF model
ncf_model_path = '/content/drive/MyDrive/Recommendation/success/ncf_model_tuned.h5'
final_ncf_model.save(ncf_model_path)
print(f"\n✓ Saved tuned NCF model to: {ncf_model_path}")

ncf_scaler_path = '/content/drive/MyDrive/Recommendation/success/ncf_scaler_tuned.pkl'
with open(ncf_scaler_path, 'wb') as f:
    pickle.dump(scaler_ncf, f)
print(f"✓ Saved NCF scaler to: {ncf_scaler_path}")

tuning_results['ncf'] = {
    'best_params': best_ncf_config,
    'best_rmse': best_ncf_rmse
}

# ============================================================
# 5. RNN MODEL TUNING
# ============================================================
print("\n" + "="*80)
print("STEP 6: TUNING RNN MODEL (Sequential Patterns)")
print("="*80)

train_df_sorted = train_df.sort_values(['userId', 'rating_time'])
test_df_sorted = test_df.sort_values(['userId', 'rating_time'])

def create_rating_sequences(df, seq_length):
    user_seqs = []
    movie_seqs = []
    target_movies = []
    target_ratings = []

    for user_id in df['userId'].unique():
        user_data = df[df['userId'] == user_id].sort_values('rating_time')

        if len(user_data) < 2:
            continue

        user_movies = user_data['movie_id_encoded'].values
        user_ratings = user_data['rating'].values
        user_encoded = user_data['user_id_encoded'].values[0]

        for i in range(1, len(user_movies)):
            movie_seq = user_movies[max(0, i-seq_length):i]
            target_movie = user_movies[i]
            target_rating = user_ratings[i]

            user_seqs.append(user_encoded)
            movie_seqs.append(movie_seq)
            target_movies.append(target_movie)
            target_ratings.append(target_rating)

    return user_seqs, movie_seqs, target_movies, target_ratings

def build_rnn_model(sequence_length, embedding_dim, rnn_units, dense_units,
                    learning_rate, dropout_rate, n_users, n_movies):
    keras.backend.clear_session()

    user_input = layers.Input(shape=(1,), name='user_input')
    movie_seq_input = layers.Input(shape=(sequence_length,), name='movie_seq_input')
    target_movie_input = layers.Input(shape=(1,), name='target_movie_input')

    user_emb = layers.Embedding(n_users, embedding_dim, name='user_embedding')(user_input)
    user_emb = layers.Flatten()(user_emb)

    movie_seq_emb = layers.Embedding(n_movies, embedding_dim, name='movie_seq_embedding')(movie_seq_input)
    rnn_out = layers.GRU(rnn_units, return_sequences=False, name='gru')(movie_seq_emb)

    target_movie_emb = layers.Embedding(n_movies, embedding_dim, name='target_movie_embedding')(target_movie_input)
    target_movie_emb = layers.Flatten()(target_movie_emb)

    concat = layers.Concatenate()([user_emb, rnn_out, target_movie_emb])

    x = concat
    for units in dense_units:
        x = layers.Dense(units, activation='relu')(x)
        x = layers.Dropout(dropout_rate)(x)

    output = layers.Dense(1, activation='sigmoid', name='rating_output')(x)

    model = keras.Model(
        inputs=[user_input, movie_seq_input, target_movie_input],
        outputs=output
    )

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='mse',
        metrics=['mae']
    )

    return model

# RNN configurations to test
rnn_configs = [
    {'sequence_length': 10, 'embedding_dim': 50, 'rnn_units': 128, 'dense_units': [256, 128], 'learning_rate': 0.001, 'dropout_rate': 0.3},
    {'sequence_length': 15, 'embedding_dim': 50, 'rnn_units': 128, 'dense_units': [256, 128], 'learning_rate': 0.001, 'dropout_rate': 0.2},
    {'sequence_length': 10, 'embedding_dim': 100, 'rnn_units': 128, 'dense_units': [256, 128], 'learning_rate': 0.0005, 'dropout_rate': 0.3},
    {'sequence_length': 10, 'embedding_dim': 50, 'rnn_units': 256, 'dense_units': [512, 256], 'learning_rate': 0.001, 'dropout_rate': 0.3},
]

print(f"\nTesting {len(rnn_configs)} RNN configurations...")
print("This may take 30-45 minutes...\n")

best_rnn_rmse = float('inf')
best_rnn_config = None
best_rnn_model = None
best_rnn_scaler = None

for i, config in enumerate(rnn_configs):
    print(f"\nConfiguration {i+1}/{len(rnn_configs)}:")
    print(f"  Seq length: {config['sequence_length']}, Embedding: {config['embedding_dim']}, RNN units: {config['rnn_units']}")
    print(f"  Dense: {config['dense_units']}, LR: {config['learning_rate']}, Dropout: {config['dropout_rate']}")

    # Create sequences
    user_seqs, movie_seqs, target_movies, target_ratings = create_rating_sequences(
        train_df_sorted, config['sequence_length']
    )

    X_users = np.array(user_seqs)
    X_movies = pad_sequences(movie_seqs, maxlen=config['sequence_length'], padding='pre', value=0)
    X_target_movies = np.array(target_movies)
    y = np.array(target_ratings)

    # Normalize
    scaler_rnn = MinMaxScaler()
    y_scaled = scaler_rnn.fit_transform(y.reshape(-1, 1)).flatten()

    # Split
    split_idx = int(len(X_users) * 0.9)
    X_users_train = X_users[:split_idx]
    X_movies_train = X_movies[:split_idx]
    X_target_train = X_target_movies[:split_idx]
    y_train_rnn = y_scaled[:split_idx]

    X_users_val = X_users[split_idx:]
    X_movies_val = X_movies[split_idx:]
    X_target_val = X_target_movies[split_idx:]
    y_val_rnn = y_scaled[split_idx:]

    # Build and train model
    model = build_rnn_model(
        config['sequence_length'],
        config['embedding_dim'],
        config['rnn_units'],
        config['dense_units'],
        config['learning_rate'],
        config['dropout_rate'],
        n_users,
        n_movies
    )

    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True,
        verbose=0
    )

    history = model.fit(
        [X_users_train, X_movies_train, X_target_train],
        y_train_rnn,
        batch_size=256,
        epochs=20,
        validation_data=([X_users_val, X_movies_val, X_target_val], y_val_rnn),
        callbacks=[early_stopping],
        verbose=0
    )

    # Evaluate
    val_pred = model.predict([X_users_val, X_movies_val, X_target_val], verbose=0)
    val_pred_unscaled = scaler_rnn.inverse_transform(val_pred).flatten()
    y_val_unscaled = scaler_rnn.inverse_transform(y_val_rnn.reshape(-1, 1)).flatten()

    rmse = math.sqrt(mean_squared_error(y_val_unscaled, val_pred_unscaled))
    mae = mean_absolute_error(y_val_unscaled, val_pred_unscaled)
    print(f"  Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}")
    print(f"  Epochs trained: {len(history.history['loss'])}")

    if rmse < best_rnn_rmse:
        best_rnn_rmse = rmse
        best_rnn_config = config
        best_rnn_model = model
        best_rnn_scaler = scaler_rnn
        print("  ✓ New best RNN model!")

# After loop finishes
print(f"\n✓ Best RNN RMSE: {best_rnn_rmse:.4f}")
print(f"✓ Best RNN configuration: {best_rnn_config}")

# ------------------------------------------------------------
# ✅ Retrain best RNN model on FULL sequential dataset
# ------------------------------------------------------------
print("\nRetraining best RNN model on full sequential data...")

full_user_seqs, full_movie_seqs, full_target_movies, full_target_ratings = create_rating_sequences(
    train_df_sorted, best_rnn_config['sequence_length']
)

X_users_full = np.array(full_user_seqs)
X_movies_full = pad_sequences(full_movie_seqs, maxlen=best_rnn_config['sequence_length'], padding='pre', value=0)
X_target_full = np.array(full_target_movies)
y_full = np.array(full_target_ratings)

y_full_scaled = best_rnn_scaler.fit_transform(y_full.reshape(-1, 1)).flatten()

final_rnn_model = build_rnn_model(
    best_rnn_config['sequence_length'],
    best_rnn_config['embedding_dim'],
    best_rnn_config['rnn_units'],
    best_rnn_config['dense_units'],
    best_rnn_config['learning_rate'],
    best_rnn_config['dropout_rate'],
    n_users,
    n_movies
)

early_stopping_full = keras.callbacks.EarlyStopping(
    monitor='loss',
    patience=5,
    restore_best_weights=True,
    verbose=0
)

final_rnn_model.fit(
    [X_users_full, X_movies_full, X_target_full],
    y_full_scaled,
    batch_size=256,
    epochs=30,
    callbacks=[early_stopping_full],
    verbose=1
)

# ------------------------------------------------------------
# ✅ Save final tuned RNN model + scaler + params
# ------------------------------------------------------------
rnn_model_path = '/content/drive/MyDrive/Recommendation/success/rnn_model_tuned.h5'
final_rnn_model.save(rnn_model_path)

with open('/content/drive/MyDrive/Recommendation/success/rnn_scaler_tuned.pkl', 'wb') as f:
    pickle.dump(best_rnn_scaler, f)

with open('/content/drive/MyDrive/Recommendation/success/rnn_params_tuned.pkl', 'wb') as f:
    pickle.dump({'sequence_length': best_rnn_config['sequence_length']}, f)

print(f"\n✓ Saved tuned RNN model: {rnn_model_path}")
print("✓ Saved RNN scaler & parameters")

tuning_results['rnn'] = {
    'best_params': best_rnn_config,
    'best_rmse': best_rnn_rmse
}

# ------------------------------------------------------------
# ✅ Save final summary
# ------------------------------------------------------------
summary_path = '/content/drive/MyDrive/Recommendation/success/tuning_results_summary.json'
with open(summary_path, 'w') as f:
    json.dump(tuning_results, f, indent=2)

print("\n================ TUNING COMPLETE ================")
print(json.dumps(tuning_results, indent=2))
print(f"Results saved to: {summary_path}")